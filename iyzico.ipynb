{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly, we will create train and test data sets. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_data = pd.read_csv(\"/kaggle/input/iyzico-projesi/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/iyzico-projesi/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us view features and targets of train and test data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, train data has 160000 has data value and 12 features. The last column indicates the target values. The column of `ISFLAUD` is target column."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nimport seaborn as sn\n\ndf = DataFrame(train_data,columns=['BASKETINSTALLMENT','BASKETPAYMENTCHANNEL','BASKETPAYMENTSOURCETYPE',\n                                  'BASKETISTHREEDS', 'BASKETREGISTERCARD', 'BASKETHASVIRTUALITEM', \n                                  'CARDTYPE', 'CARDASSOCIATION'])\n\ncorrMatrix = df.corr()\nsn.heatmap(corrMatrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the data set, the features of `EMAIL`, `CARDBANKID`, `MERCHANT_ID` have not huge effects on prediction of transactions. Because, we don't interest persons who perform transactions, interest only process of transaction. Therefore, we can drop these columns from the train data set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop([\"EMAIL\", \"MERCHANT_ID\", \"CARDBANKID\"], axis= 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we check `NaN` values whether exist or not in the data set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor row in range(train_data.shape[0]):\n    for column in range(train_data.shape[1]):\n        if pd.isnull(train_data.iloc[row, column]) == True:\n            count += 1\n        else:\n            continue\n\nif count == 0:\n    print('There is no NaN value in the data set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can process the clean data. We will Recurrent Neural Network for predicstion. In the Recurrent Neural Network, the method which we use for prediction is LSTM. "},{"metadata":{},"cell_type":"markdown","source":"Before struct the network, let us split data(train.csv) for train and test. We will control the strength of the model based on accuracy and loss. Then, we will use updated weigths with the neural network for prediction of data values in the test(test.csv) data. "},{"metadata":{},"cell_type":"markdown","source":"In the data set, some features have '0' categorical values. This situation may bring about some problems for compiling the model. So, we will add 1 to all values of some features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train_data.iloc[:, :-1]\ntarget = train_data.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.preprocessing import MinMaxScaler\n\n#minmax_scaler = MinMaxScaler()\n#train_data['BASKETPAYMENTCHANNEL'] = minmax_scaler.fit_transform(train_data['BASKETPAYMENTCHANNEL'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature_set = ['BASKETINSTALLMENT', 'BASKETPAYMENTCHANNEL', \n#               'BASKETPAYMENTSOURCETYPE', 'BASKETISTHREEDS', 'BASKETREGISTERCARD', \n#               'BASKETHASVIRTUALITEM', 'CARDTYPE', 'CARDASSOCIATION']\n\n#for feature_index in features:\n#    train_data[feature_index] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nfrom sklearn.model_selection import train_test_split\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.33, random_state = random.randrange(0,100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\n\nfor index in range(len(test_target)):\n    if target[index] == 1:\n        count += 1\n        \nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to expand dimension of the train and test features in order to construct suitable network for the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = np.expand_dims(train_features, axis = 2)\ntest_features = np.expand_dims(test_features, axis = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras \n\ntrain_target = keras.utils.to_categorical(train_target)\ntest_target = keras.utils.to_categorical(test_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us create the network. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv1D, MaxPooling1D, LSTM\nfrom keras.models import Sequential \nfrom keras.regularizers import l1, l2, l1_l2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is input layer of the network."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are four hidden layers in the network. `input_shape` indicates the shape of previous layer. In the first hidden layer, `input_shape` must be assigned as dimension of input layer. The `ReLu` function is used as non-linear activation function in all hidden layers. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.add(Conv1D(64, 2, input_shape = (26, 1), activation = 'relu', kernel_regularizer='l1_l2'))\n\n#model.add(LSTM(64, return_sequences=True, input_shape=(train_features.shape[1], 1), \n#               activation='relu', kernel_regularizer = 'l2'))\n\nmodel.add(BatchNormalization())\nmodel.add(Dense(850, input_shape=(train_features.shape[1], 1), activation = 'relu', kernel_regularizer='l2'))\nmodel.add(Dropout(0.5))\n#model.add(BatchNormalization())\nmodel.add(Dense(900, activation = 'tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(850, activation = 'relu'))\n\nmodel.add(Flatten())\n#model.add(LSTM(50, return_sequences=True, activation='relu'))\n#model.add(LSTM(50, return_sequences=True, activation='tanh'))\n#model.add(LSTM(50, return_sequences=False, activation='tanh'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final stage of constructing the architecture of the network is setting output layer. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.add(Dropout(0.5))\n#model.add(BatchNormalization())\nmodel.add(Dense(2, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We used `l2 regularization` at output layer and one hidden layer to reduce effect of overfitting. Because, a gap between train and test curves has occured without `l2 regularization`. The gap was reduced with `l2 regularization`. "},{"metadata":{},"cell_type":"markdown","source":"We constructed the network. Now, we must compile with optimizer and loss functions "},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_param = keras.optimizers.Adam(learning_rate = 0.0001)\nmodel.compile(loss='binary_crossentropy', optimizer = opt_param, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Default value of the learning rate is reduced to 0.000001 because the loss and the accuracy values did not change during epochs. This means that the model could not learn anything from data. "},{"metadata":{},"cell_type":"markdown","source":"At the final stage, we are fitting with paramaters of `batch_size` and `epoch`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ann = model.fit(train_features, train_target, batch_size=1024, epochs=5, validation_data=(test_features, test_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the Loss Curves\nplt.figure(figsize=[8,6])\ntrain_loss, = plt.plot(ann.history['loss'],'r',linewidth=3.0)\ntest_loss, = plt.plot(ann.history['val_loss'],'b',linewidth=3.0)\nplt.legend([train_loss, test_loss], ['Training Loss', 'Test Loss'],fontsize=12)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Loss',fontsize=16)\nplt.title('Loss Curves',fontsize=16)\n \n#Plot the Accuracy Curves\nplt.figure(figsize=[8,6])\ntrain_accuracy, = plt.plot(ann.history['accuracy'],'r',linewidth=3.0)\ntest_accuracy, = plt.plot(ann.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend([train_accuracy, test_accuracy], ['Training Accuracy', 'Test Accuracy'],fontsize=12)\n\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end of 40 epochs, the validation accuracy is around 0.95 and the validation loss is 0.66. The curves of train and test are close to each other and there is no considerable gap between two curves in both accuracy and loss values. Therefore, we can say that there is no overfitting or underfitting. This model can be used for predicting. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop([\"EMAIL\", \"MERCHANT_ID\", \"CARDBANKID\",\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = np.expand_dims(test_data, axis = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test data is ready for predicting. Let us predict target values with the network model which we created. "},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to classes for `ISFRAUD`. So, we use `predict_classes` function belongs to the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_classes = model.predict_classes(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = list(prediction_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 0 in predictions:\n    print('yes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a data frame, then write this data frame to csv file. "},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.DataFrame(columns = ['ID', 'ISFRAUD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction_id = []\n\nfor index in range(0, len(predictions)):\n    transaction_id.append(index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df['ID'] = transaction_id\nresult_df['ISFRAUD'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the last submission file is created."},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.to_csv('submissions15.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}